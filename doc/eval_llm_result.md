# Eval_LLM

This doc aims to summarize the performance of publicly available big models when evaluated on the spider dataset. We hope it will provide a point of reference for folks using these big models for Text-to-SQL tasks. We'll keep sharing eval results from models we've tested and seen others use, and we welcome any contributions to make this more comprehensive.

## 1.LLMs text-to-sq1 capability evaluation
| name        | Execution Accuracy | reference                                                          |
| ----------- | ------------------ | ------------------------------------------------------------------ |
| ChatGPT     | 0.728              | [quote](https://www.numbersstation.ai/post/nsql-llama-2-7b)         |
| GPT 4       | 0.762              | [quote](https://www.numbersstation.ai/post/nsql-llama-2-7b)         |
| wizardcoder | 0.610              | [quote](https://github.com/cuplv/text-to-sql-wizardcoder/tree/main) |



## 2. Acknowledgements
Thanks to the following open source projects.

*  [text-to-sql-wizardcoder](https://github.com/cuplv/text-to-sql-wizardcoder)
*  [nsql-llama-2-7b](https://www.numbersstation.ai/post/nsql-llama-2-7b)
